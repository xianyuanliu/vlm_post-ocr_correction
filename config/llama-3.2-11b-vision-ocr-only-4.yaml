settings:
  data: 'data/'
  model: 'meta-llama/Llama-3.2-11B-Vision'
  adapter: 'model/llama-3.2-11b-vision-ocr-only-4'
  chat_template: "{{- bos_token }}{% for message in messages %}{% if loop.first and message.role == 'system' %}<|start_header_id|>system<|end_header_id|>{{ \"\\n\\n\" }}{% for item in message.content %}{% if item.type == 'text' %}{{ item.text }}{% endif %}{% endfor %}<|eot_id|>{% else %}{% if loop.index0 % 2 == 1 and message.role == 'user' %}<|start_header_id|>user<|end_header_id|>{{ \"\\n\\n\" }}{% for item in message.content %}{% if item.type == 'image' %}<|image|>{% elif item.type == 'text' %}{{ item.text }}{% endif %}{% endfor %}<|eot_id|>{% elif loop.index0 % 2 == 0 and message.role == 'assistant' %}<|start_header_id|>assistant<|end_header_id|>{{ \"\\n\\n\" }}{% for item in message.content %}{% if item.type == 'text' %}{{ item.text }}{% endif %}{% endfor %}<|eot_id|>{% endif %}{% endif %}{% endfor %}{% if add_generation_prompt %}<|start_header_id|>assistant<|end_header_id|>{{ \"\\n\\n\" }}{% else %}{{- eos_token }}{% endif %}"
  freeze_img: false
  use_ocr: true
  use_img: false

lora:
  lora_alpha: 16
  lora_dropout: 0.05
  r: 8
  bias: 'none'
  target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
  task_type: 'CAUSAL_LM'

train:
  num_train_epochs: 1
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 8
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false
  optim: 'adamw_torch_fused'
  learning_rate: 2.0e-4
  lr_scheduler_type: 'constant'
  save_strategy: 'no'
  eval_strategy: 'steps'
  logging_steps: 20
  eval_steps: 20
  bf16: true
  tf32: true
  max_grad_norm: 0.3
  warmup_ratio: 0.03
  dataset_text_field: ''
  dataset_kwargs:
    skip_prepare_dataset: true
  remove_unused_columns: false
  max_seq_length: 1024
  report_to: 'wandb'
  seed: 4
